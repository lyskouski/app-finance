% Copyright 2023 The terCAD team. All rights reserved.
% Use of this content is governed by a CC BY-NC-ND 4.0 license that can be found in the LICENSE file.

\subsection{Testing Quality}
\markboth{Gating}{Testing Quality} \label{quality}

Testing is an indispensable aspect (robustness, reliability, and even user satisfaction) of any development process 
through a rigorous examination of application capabilities.

Different layers of tests \emph{(end-to-end business scenarios, exploratory [security] and benchmark [performance, scalability], 
user [UI] and application programming [API] interfaces, system [for a distributed architecture], integration, component,
and unit testing)} act as guardians of an application integrity per each dimension \emph{(from a full ecosystem to a 
single method)} to rectify issues (bugs or logic errors) on early stages before a release. Thus reduce the risk of 
costly and time-consuming repairs later. To be more precise, the cost of fixing bugs is exponential through the
product life-cycle (architecture, coding, integration, system distribution, production, post-release) 
\cite{Sanket19} \cite{Boeh88}, and an architectural mistake might cost \q{N x 30} on post-release phase, where 
\q{N} -- is an amount of totally spent money for that feature.

By subjecting the application to a diverse range of test cases and edge cases, developers can ensure its stability and 
resilience; moreover, tests promote flexibility and maintainability, facilitating continuous integration and deployment 
practices. That's why agility (survivability) of the project relies on a tests automation by swift feedback loops.

Finally, tests inspire confidence among stakeholders, end-users, and clients. As an example, security testing helps 
identify weaknesses and potential attack vectors, mitigating risks and protecting sensitive user data. This confidence 
strengthens the application's reputation and fosters user loyalty, leading to increased adoption and market success.\\
\\

\noindent Going back to our project... we've created some functionality (business logic) with a not so trivial flow. 
Not to waste a time on checking it manually in repetitive cycles after each change (and, mostly, after 
refactoring), let's create some tests to check it for us automatically.


\subsubsection{Introducing Unit Tests} \label{ut-unit}

Let's declare what unit testing is. Unit tests are focused on verifying the correctness of a functionality for 
individual units (or components); and by "individual"-word it's meant that no any other processes take place in the 
system (expected isolation [simulated behavior of dependent units] and deterministic results [the same input always 
yield the same output]). Unit tests are also known as "white box" approach, but would recommend to ignore that 
statement and follow Behavior Driven Development (BDD) rule -- assert user requirements and expectations instead of 
functional coverage (by testing conditional statements that's written inside the targeted unit) \cite{Crispin09}.

In Flutter, \q{test}-function from the \q{flutter\_test}- package is utilized for writing unit tests. By writing them, 
all the defined function calls within files following the *\_test.dart pattern are automatically recognized by the 
\q{flutter test}-command line execution. Alternatively, all tests are shown in "Test Explorer" of IDE and can be
evaluated solely, by group, by file, or fully (\cref{img:fs-test-explorer}).

\img{prototyping/test-explorer}{Test Explorer in Visual Studio Code}{img:fs-test-explorer}

\noindent As a first step, we would start from covering the implemented Account Recalculation flow:

\begin{lstlisting}
// test/unit/_classes/data/account_recalculation_test.dart 
void main() {
  // 'group' works as a prefix to all the tests inside
  group('Account Recalculation:getDelta', () {
    late AccountRecalculation mock; // Deferred initialization 
    // 'setUp' - is going to be executed before each test
    // 'setUpAll' - once at the beginning of group evaluation
    setUp(() {
      final accountMock = AccountAppData(
        uuid: '1',
        title: 'test',
        type: AppAccountType.account.toString(),
      );
      mock = AccountRecalculation(
        initial: accountMock.clone(),
        change: accountMock.clone(),
      );
    });
    // Define "table" of input arguments and output assertions
    final testCases = [
      (
        initial: (hidden: false, details: 5.0),
        change: (hidden: false, details: 1.0),
        result: -4.0
      ),
      // ... other variations
      (
        initial: null,
        change: (hidden: false, details: 2.0),
        result: 2.0
      )
    ];
    // Run all defined Test Cases
    for (var v in testCases) {
      test('$v', () {
        // Update inputs
        if (v.initial != null) {
          object.initial!.hidden = v.initial!.hidden;
          object.initial!.details = v.initial!.details;
        } else {
          object.initial = null;
        }
        object.change.hidden = v.change.hidden;
        object.change.details = v.change.details;
        // Assert 'AccountRecalculation.getDelta'-method results
        expect(object.getDelta(), v.result);
      });
    }
  });
}
\end{lstlisting}

\noindent By typing \q{flutter test} in the command line we would see something like:

\begin{lstlisting}
[V] Given Main page When tap on Create Then opened BillAddPage
[V] Account Recalculation:getDelta (change: (details: 1.0, hidden: false), initial: (details: 5.0, hidden: false), result: -4.0)
[V] Account Recalculation:getDelta (change: (details: 5.0, hidden: false), initial: (details: 1.0, hidden: false), result: 4.0)
Expected: <3.0>
  Actual: <1.0>
[X] Account Recalculation:getDelta (change: (details: 3.0, hidden: false), initial: (details: 2.0, hidden: true), result: 3.0)
[V] Account Recalculation:getDelta (change: (details: 3.0, hidden: true), initial: (details: 2.0, hidden: false), result: -2.0)
Null check operator used on a null value
[X] Account Recalculation:getDelta (change: (details: 2.0, hidden: false), initial: null, result: 2.0)
Exited (1)
\end{lstlisting}

\noindent That test is highlighted a dozen of troubles and failures in our code, so, it definitely has not been 
written in vain.

\begin{lstlisting}[firstnumber=17]
// ./lib/_classes/data/account_recalculation.dart
double getDelta() {
  return change.hidden
\end{lstlisting}
{
\xpretocmd{\lstlisting}{\vspace{-12pt}}{}{}
\begin{lstlisting}[firstnumber=20, backgroundcolor=\color{backred}]
(*@\kdiff{-}@*)  ? -initial!.details
\end{lstlisting}
\begin{lstlisting}[firstnumber=20, backgroundcolor=\color{backgreen}]
(*@\kdiff{+}@*)  ? -(initial?.details ?? 0.0)
\end{lstlisting}
\begin{lstlisting}[firstnumber=21, backgroundcolor=\color{backred}]
(*@\kdiff{-}@*)  : change.details - initial!.details;
\end{lstlisting}
\begin{lstlisting}[firstnumber=21, backgroundcolor=\color{backgreen}]
(*@\kdiff{+}@*)  : (initial?.hidden ?? true
(*@\kdiff{+}@*)      ? change.details
(*@\kdiff{+}@*)      : change.details - initial?.details);
\end{lstlisting}
\begin{lstlisting}[firstnumber=24]
}
\end{lstlisting}
}


\subsubsection{Creating Widget Tests} \label{widget-tests}

Widget tests in Flutter are a form of automated testing used to verify the correctness and behavior of individual 
widgets in an application. These tests are focused on evaluating the appearance, layout, and interaction of widgets, 
ensuring that they are rendered correctly and respond appropriately to user interactions.

\noindent ... and we've already broken a first of them (that's been provided through the Flutter template generation). 
At the beginning it was covering \q{Add}-button as an incremental trigger for the shown counter.

\begin{lstlisting}
void main() {
  testWidgets('Counter increments', (WidgetTester tester) async {
    // Build our app and trigger a frame.
    await tester.pumpWidget(const MyApp());
    // Verify that our counter starts at 0.
    expect(find.text('0'), findsOneWidget);
    expect(find.text('1'), findsNothing);
    // Tap the '+' icon and trigger a frame.
    await tester.tap(find.byIcon(Icons.add));
    await tester.pump();
    // Verify that our counter has incremented.
    expect(find.text('0'), findsNothing);
    expect(find.text('1'), findsOneWidget);
  });
}
\end{lstlisting}

\noindent By now, let's just check that \q{Add}-button is opening a Widget with a three named tabs:

\begin{lstlisting}
void main() {
  testWidgets('Given Main page When tap on Create Then opened BillAddPage', (WidgetTester tester) async {
    // App initialization
    await tester.pumpWidget(
        // Taken as is from 'void main' of 'lib/main.dart'-file
        ChangeNotifierProvider(
            create: (_) => AppData(),
            child: const MyApp(),
        )
    );
    // Checking one of the text widgets
    expect(find.text('Goals'), findsOneWidget);
    // Clicking on 'Add'-button
    await tester.tap(find.byIcon(Icons.add));
    // Waiting till finished render state
    await tester.pumpAndSettle();
    // Asserting icons on our tabs
    expect(find.byIcon(Icons.insert_invitation), findsOneWidget);
    expect(find.byIcon(Icons.money_off), findsOneWidget);
    expect(find.byIcon(Icons.transform), findsOneWidget);
  });
}
\end{lstlisting}

\noindent Here we are, by \q{tap} and \q{expect} the flow of the whole application can be checked. Such type of tests 
is called as Widget tests that's emphasized by specially named \q{testWidgets}-function.

Initially, it would be better to concentrate mostly on Business Logic check (by unit tests) than on a draft state of 
our pages and their flow. Nonetheless, the current one will check that the application can be started without errors, 
and there're no any freezes or an infinite loops of rendering (at least for those two pages). 


\subsubsection{Using Code Generators} \label{ut-code-generator}

We've implemented earlier (\ref{ut-unit}) the test for a simple function that is isolated by itself from all other 
functionality. In most of the cases our functions are calling other functions, while unit test has to check the logic 
of the defined function isolately by cutting all external interactions (idealy, even within one class). To achieve that 
we might use \q{Mockito} (\href{https://github.com/dart-lang/mockito}{https://github.com/dart-lang/mockito}):

\begin{lstlisting}
// ./test/unit/_classes/data/account_recalculation_test.dart
// Original class that we're testing
import 'package:app_finance/_classes/data/account_recalculation.dart';
// Wrapping our class by Mockito
@GenerateNiceMocks([MockSpec<AccountRecalculation>()])
import 'account_recalculation_test.mocks.dart';

void main() {
  group('AccountRecalculation updateGoals', () {
    late List<GoalAppData> goals;
    // Data initialization
    setUp(() {
      goals = [
        GoalAppData(title: '1', details: 25.0, progress: 0.0),
        GoalAppData(title: '2', details: 50.0, progress: 0.0),
        GoalAppData(title: '3', details: 100.0, progress: 0.0),
      ];
    });
    // Test Suite (Behavioral Matrix)
    final testCases = [
      (getDelta: 0.0, progress: [0.0, 0.0, 0.0], result: [0.0, 0.0, 0.0]),
      // ... other cases
    ];
    for (var v in testCases) { // Loop across all Test Cases
      test('$v', () {// Make test representation named by its inputs
        final mock = MockAccountRecalculation();
        // Wrap "external" method
        when(mock.getDelta()).thenReturn(v.getDelta);
        // Preparing data for the test
        for (int i = 0; i < v.progress.length; i++) {
          goals[i].progress = v.progress[i];
        }
        mock.updateGoals(goals);
        // Asserting results
        for (int i = 0; i < v.result.length; i++) {
          expect(goals[i].progress, v.result[i]);
        }
// ... closing brackets
\end{lstlisting}

\noindent Almost done... failing since \q{account\_recalculation\_test.mocks.dart}-file is missing. To resolve that we 
have to use additional library \q{build\_runner} (\q{flutter pub add build\_runner --dev}) that will generate 
\q{*.mocks.dart}-files for us. But that makes our work harder since before each \q{flutter test} we have to run 
\q{flutter run build\_runner build}. Let's cover that by Grinder task (\ref{a-grinder}):

\begin{lstlisting}
// ./tool/grind.dart
@Task('Generate Mocks')
mock() {
  ProcessResult build = Process.runSync('dart', [
    'run',
    'build_runner',
    'build',
    '--delete-conflicting-outputs',
  ]);
  if (build.exitCode > 0) {
    fail(build.stderr);
  }
}

@Task('Run tests')
@Depends(mock)
test() {
  TaskArgs args = context.invocation.arguments;
  ProcessResult test = Process.runSync(
    'flutter', 
    [
      'test',
      args.getFlag('coverage') ? '--coverage' : '',
      args.getOption('path') ?? ''
    ].where((e) => e != '').toList(), // Drop empty arguments
    // To avoid error "The system cannot find the file specified"
    runInShell: true
  );
  if (test.exitCode > 0) {
    fail(test.stderr);
  }
  log(test.stdout);
}
\end{lstlisting}

\noindent Let's type in the console our new command for a tests execution (as a note, it can be used once to generate 
files and then proceed with \q{flutter test}-command):

\begin{lstlisting}[language=terminal]
$ dart run grinder test
  [INFO] Generating build script...
  ...
  [INFO] Succeeded after 51ms
  00:00 +0: AccountRecalculation updateGoals (getDelta: 0.0, ...
  ...
  00:01 +7: All tests passed!
\end{lstlisting}

\noindent The first test with mocks passed, but not actually evaluated the original method as we might assume, 
everything is mocked! To achieve a capability of a partial mocking we would need to implement a wrapper around the 
original class. By using \q{mockGetDelta}-method it would be possible to override (mock) behavior of any method. 

\begin{lstlisting}
class WrapperAccountRecalculation extends AccountRecalculation {
  double Function()? _getDelta;
  // ignore: non_constant_identifier_names
  set mockGetDelta(double Function() value) {
    _getDelta = value;
  }

  @override
  double getDelta() => (_getDelta ?? super.getDelta)();
}
\end{lstlisting}

\noindent Let's automate that part as well by using \q{source\_gen}-package, that enables a code generation based on 
annotations (we've seen already - \q{@GenerateNiceMocks}). Firstly, our custom builder is needed to be registered via 
\q{build.yaml}-file:

\begin{lstlisting}[language=yaml]
## ./build.yaml
targets: # Force to activate our generator
  $default:
    builders: 
      # Combination of application name and builder definition
      app_finance|wrapper_generator:
        enabled: true # Activate its usage
builders:
  wrapper_generator: # Builder name
    # Builder file location
    import: package:app_finance/_classes/gen/wrapper_generator.dart
    # Method name that is used as a factory
    builder_factories: ["wrapperGenerator"]
    build_extensions: {".dart": [".wrapper.dart"]}
    auto_apply: dependents
    build_to: source # Generate to file
    applies_builders: ["source_gen|combining_builder"]
\end{lstlisting}

\begin{lstlisting}
// ./lib/_classes/gen/generate_with_method_setters.dart
class GenerateWithMethodSetters {
  final List<Type> classes;
  const GenerateWithMethodSetters([this.classes = const []]);
}
\end{lstlisting}
\begin{lstlisting}
// ./lib/_classes/gen/wrapper_generator.dart
Builder wrapperGenerator(BuilderOptions options) => LibraryBuilder(
      WrapperGenerator(),
      generatedExtension: '.wrapper.dart',
    );

class WrapperGenerator extends Generator {
  String build(Iterable<DartObject> annotations) {
    final result = StringBuffer();
    for (final annotation in annotations) {
      final classes = annotation.getField('classes');
      if (classes!.isNull) {
        break;
      }
      for (final name in classes.toListValue()!) {
        final type = name.toTypeValue();
        final classElement = type?.element as ClassElement;
        final visitor = WrapperVisitor(classElement);
        result.writeln(visitor.toString());
      }
    }
    return result.toString();
  }
  @override
  String generate(LibraryReader library, BuildStep buildStep) {
    final result = StringBuffer();
    for (final element in library.allElements) {
      final annotations =
          const TypeChecker.fromRuntime(GenerateWithMethodSetters)
              .annotationsOf(element);
      if (annotations.isNotEmpty) {
        result.writeln(build(annotations));
      }
    }
    return result.toString();
  }(*@ \stopnumber @*)
}

// ./lib/_classes/gen/wrapper_visitor.dart
class WrapperVisitor {
  StringBuffer buffer = StringBuffer();

  ClassElement element;

  WrapperVisitor(this.element);

  void addImports() {
    final mainClass = element.enclosingElement.library;
    addImport(mainClass);
    for (final cls in mainClass.importedLibraries) {
      addImport(cls);
    }
    buffer.writeln('');
  }

  void addImport(LibraryElement className) {
    buffer.writeln('// ignore: unused_import');
    buffer.writeln("import '${className.source.uri}';");
  }

  void addClassDefinition() {
    buffer.writeln('class Wrapper${element.name} extends ${element.name} {');
    final constructor = element.unnamedConstructor;
    if (!constructor!.isDefaultConstructor) {
      final properties = constructor.parameters;
      buffer.writeln('  Wrapper${element.name}({');
      if (properties.isNotEmpty) {
        for (var e in properties) {
          buffer.writeln(
            '${e.isRequired ? 'required ' : ''}super.${e.name},');
        }
      }
      buffer.writeln('  });');
    }
  }

  void addMethods() {
    for (final m in element.methods) {
      final args = m.parameters.map((e) => e.name).toList().join(', ');
      final name = 'mock${m.name[0].toUpperCase()}${m.name.substring(1)}';
      buffer.writeln('');
      buffer.writeln('${m.returnType} Function()? _${m.name};');
      buffer.writeln('// ignore: non_constant_identifier_names');
      buffer.writeln('set $name(${m.returnType} Function() val) {');
      buffer.writeln('    _${m.name} = val;');
      buffer.writeln('}');
      buffer.writeln('');
      buffer.writeln('@override');
      buffer.writeln('$m => (_${m.name} ?? super.${m.name})($args);');
    }
  }

  void finalize() => buffer.writeln('}');

  @override
  String toString() {
    addImports();
    addClassDefinition();
    addMethods();
    finalize();
    return buffer.toString();
  }
}
\end{lstlisting}

\noindent Lastly, we need to adjust the test to accommodate the implemented wrapper:

\begin{lstlisting}
@GenerateWithMethodSetters([AccountRecalculation])
import 'account_recalculation_test.wrapper.dart';

test('Demo', () {
  final mock = MockAccountRecalculation();
  final wrapper = WrapperAccountRecalculation();
  when(mock.getDelta()).thenReturn(v.getDelta);
  wrapper.mockGetDelta = mock.getDelta;
  for (int i = 0; i < v.progress.length; i++) {
    goals[i].progress = v.progress[i];
  }
  wrapper.updateGoals(goals);
  verify(mock.getDelta()).called(1);
  for (int i = 0; i < v.result.length; i++) {
    expect(goals[i].progress, v.result[i]);
  }
});
\end{lstlisting}


\subsubsection{Adding Behavioral Tests (Gherkin)} \label{t-gherkin}

Improvement cycles are never end. Previously (\ref{widget-tests}), we've discussed the approach to test widgets and 
applied \q{When...Given...Then...}-notation. That notation is a part of Behavior-Driven Development (BDD) -- the 
process of approaching the outcome before the code. Building on that concept, a test typically 
begins with a description of the initial state (When). Following this, we declare an action (Given), and conclude by 
outlining the expected result (Then). We've used that notation as a comment that is useless without any interpreter 
(\emph{since it can be written and tested in different flows; or, it's been just forgotten to update the comment 
through the time of a project evolution}).

Cucumber (\href{https://docs.cucumber.io}{https://docs.cucumber.io}) is a practical standard, by being widely used, 
and testing framework for Behavior Driven Development approach. Gherkin 
(\href{https://pub.dev/packages/gherkin}{https://pub.dev/packages/gherkin}), as a fully featured parser and 
test runner for Flutter and Dart, would be used by us to convert When / Given / Then (And / But) into a real test steps.

In Cucumber-JVM (as well as in Gherkin) calling steps from step definitions is not supported; this is by design. 
That "by design" limitation is made to not create a mess, with an assumption, that this may lead to an explosion of step 
definitions, code duplication, and high maintenance costs. We won't argue with them (recalling the repercussions of an 
architectural mistake, \ref{quality}), since a community has already tried, and we'll take all the risks to our own, 
since the testing is a variation of an art where a common sense is a mantra to succeed. Furthermore, this approach is 
already utilized by Robot Framework (\href{https://robotframework.org}{https://robotframework.org}) where a high-level 
scenarios creation on top of others signifies the declaration of a new layer of abstractions.

But let's go step-by-step... After installing Gherkin (\q{flutter pub add flutter\_gherkin --dev}), we can start from 
\q{.feature}-files creation to describe our test scenarios.

\begin{lstlisting}[language=cucumber]
@account @currency
Feature: Verify Account functionality alignment with expectations

  Scenario: Opened Account Form
    Given I am on "Main" page
    When I tap "Account" header
    And I tap "Create" button
    Then I can see "Account Form" component

  Scenario Outline: Created different Account types
    Given Opened Account Form
    When I fill the "Account" field with <type>
    Then I should see <visible> fields
    But not <hidden> fields

    Examples: 
      | type | visible | hidden |
      | ...  |  ...    |  ...   |
\end{lstlisting}

\noindent We've created Scenario "Opened Account Form" and, by braking all the recommendations, re-used it in Scenario 
Outline for \q{Given}-step. What's that giving us? To answer that question, let's check how a billing flow might look 
like:

\begin{lstlisting}[language=cucumber]
  Scenario Outline: Added different Expenses
    Given Created different Account types
    And Created different Budget types
    And Defined Exchange Rates
    And Opened Expense Form
    When I fill the "Amount" field with <type>
    And I tap "Create" button
    Then I should see "Account <account>" deduction by <amount>
    And I should see "Budget <budget>" percentage change to <value> with left <budget_amount>
\end{lstlisting}

\noindent Such a powerful simplification cannot be ignored, it's not a mess but the extreme reusability. If something
would be changed (pipeline to open form, properties, widgets, etc.), a single scenario would be responsible for that. 
The naming convention: scenario is written in past tense, step -- in present. By following such a simple rule it would 
be always obvious the nested structure from a written sentence. 

Another drawback is that Gherkin runs User Interface tests by compiling and running the original application with 
enabled Flutter Driver Extension 
(\href{https://pub.dev/documentation/flutter_gherkin/latest/}{https://pub.dev/documentation/flutter\_gherkin/latest/}) 
that simulates user behavior. It's not something that we might be interested in by now, expecting their fast evaluation 
via simulation as it's done for widget tests. So, we're going to take an interpreter of \q{.feature}-files and use 
\q{flutter\_test}-package for all other stuff. An additional benefit from that is the tests visibility via Tests 
Terminal in IDE. It would work for us with all features enabled (to run/re-run a single, a group, or all tests) and 
we would preserve a single entry point to evaluate all of the tests.

\img{prototyping/bdd-test}{Behavioral Test evaluation}{img:p-bdd}

\noindent But let's move forward iteratively, beginning with the development of a wrapper for Gherkin to parse and 
execute \q{.feature}-files (\cref{img:p-bdd}):

\begin{lstlisting}
// ./test/e2e/e2e_test.dart
void main() {
  // Taking all our .feature files
  late Iterable<File> scope = Directory('./test/e2e')
      .listSync(recursive: true)
      .where((entity) => entity is File && 
              entity.path.endsWith('.feature')).cast<File>();
  group('Behavioral Tests', () {
    for (var file in scope) {
      testWidgets(file.path, (WidgetTester tester) async {   
        await tester.pumpWidget(ChangeNotifierProvider(
          create: (_) => AppData(),
          child: const MyApp(),
        ));
        final runner = FileRunner(file);
        await runner.init(tester);
        expect(await runner.run(), true);
      });
    }
  });(*@ \stopnumber @*)
}

// ./test/e2e/_steps/file_runner.dart
class FileRunner {
  File file;
  final language = LanguageService();
  final parser = GherkinParser();
  final featureFile = FeatureFile(RunnableDebugInformation.EMPTY());
  late FeatureFileRunner runner;
  static late WidgetTester tester;

  FileRunner(this.file) {
    language.initialise();
    runner = FeatureFileRunner(
      FlutterTestConfiguration(),
      TagExpressionEvaluator(),
      ExecutableStepIterator().aggregate(),
      ProgressReporter(),
      AggregatedHook(),
    );
  }

  Future<void> init(WidgetTester tester) async {
    FileRunner.tester = tester;
    // Parsing taken file by Gherkin
    final parserResult = await parser.parseFeatureFile(
      file.readAsStringSync(),
      '',
      ProgressReporter(),
      language,
    );
    for (final feature in parserResult.features) {
      featureFile.addChild(feature);
    }
  }

  Future<bool> run() async {
    return await runner.run(featureFile);
  }(*@ \stopnumber @*)
}

// ./test/e2e/_steps/executable_step_iterator.dart
class ExecutableStepIterator {
  final List<CustomParameter> param = <CustomParameter>[];

  ExecutableStepIterator() {
    param.addAll([
      NumParameterLower(),
      IntParameterLower(),
      StringParameterLower(),
      WordParameterLower(),
    ]);
  }
  List<ExecutableStep> _register(List<dynamic> steps) {
    return steps.map(
      (s) => ExecutableStep(GherkinExpression(s.pattern, param), s),
    ).toList();
  }
  Iterable<ExecutableStep> aggregate() {
    return _register([
      OnDefinedPage(),
      TapDefinedButton(),
      TapDefinedHeader(),
      CanSeeDefinedComponent(),
    ]);
  }(*@ \stopnumber @*)
}

// ./test/e2e/_steps/given/on_defined_page.dart
class OnDefinedPage extends Given1<String> {
  @override
  RegExp get pattern => RegExp(r"I am on {string} page");

  @override
  Future<void> executeStep(String route) async {
    // ... test step implementation
  }
}(*@ \stopnumber @*)
// ... other classes implementation: 
// TapDefinedButton, TapDefinedHeader, CanSeeDefinedComponent
\end{lstlisting}

\noindent What can be noticed (line 19-22 for \q{executable\_step\_iterator.dart}-file), is that we have to declare all 
our implementations (Given/When/Then) as a list of classes. That stuff is better to be automated by scanning 
\q{\_steps}-folder and adding all classes into the list, and we do know already (\ref{ut-code-generator}) how to 
achieve that.

Additional convention should be taken into account; as soon as we're using Scenario as a low-level step, it should be
excluded from an execution scope. Let's move such scenarios into \q{.resource}-files and cover them by generics 
(additional code generated classes per each scenario in \q{.resource}-files).

\begin{lstlisting}
// That will generate all Given classes for us from .resource-files
// that are located two levels up from the current file
@GenerateGherkinResources(['../../'])
class GivenGeneric extends Given {
  @override
  RegExp get pattern => RegExp('%step%'); // Replacement pattern for step assertion

  @override
  Future<void> executeStep() async {
    // Replacement pattern for step execution
    final step = await FileReader().getFromString('''
    %feature%
    ''');
    expect(await FileRunner(FileRunner.tester).run(step), true);
  }
}
\end{lstlisting}

\noindent Let's verify what we've gathered:

\begin{lstlisting}[language=terminal]
Shell: Running scenario: Creating different Account types Examples: (1) # :3
Shell: Running scenario: Opened Account Form # :4
Shell:    V Given I am on "Home" page # :5 took 313ms
Shell:    V When I tap "Accounts, total" header # :6 took 77ms
Shell:    V And I tap "Add Account" button # :7 took 184ms
Shell:    V Then I can see "Create new Account" component # :8 ...
Shell: PASSED: Scenario Opened Account Form # :4
Shell:    V Given Opened Account Form # :4 took 584ms
\end{lstlisting}

\noindent To conceal results from low-level steps, we can create a custom \q{ProgressReporter} that will record results 
only upon failure. We can override the behavior of the \q{printMessageLine} and \q{printMessage}-functions using a 
\q{StringBuffer}-object to control the flow.

\newpage
\begin{lstlisting}[language=terminal]
# Success
Shell: Running scenario: Creating different Account types Examples
Shell:    V Given Opened Account Form # :4 took 524ms

# Failure
Shell: Running scenario: Creating different Account types Examples
Shell: Running scenario: Opened Account Form # :4
Shell:    V Given I am on "Home" page # :5 took 261ms
Shell:    V When I tap "Accounts, total" header # :6 took 65ms
Shell:    V And I tap "Add Account" button # :7 took 172ms
Shell:    X Then I can see "Create new Account (!no)" component # :8 took 4ms  
Shell: Expected: at least one matching node in the widget tree
Shell:   Actual: _TextFinder:<zero widgets with text "Create new Account (!no)" (ignoring offstage widgets)>
Shell:    Which: means none were found but some were expected
\end{lstlisting}


\subsubsection{Recording Tests Flow} \label{t-record}

During widget tests, identifying the root cause of failure can sometimes be challenging. While the failing step and 
its assertion are clear, the absence of visual cues makes it difficult to pinpoint the issue promptly. By utilizing the 
\q{RepaintBoundary}-widget, we may effectively mimic a screen saver by wrapping the primary widget for the test:

\begin{lstlisting}
// ./test/pump_main.dart
import 'dart:ui' as ui;

class PumpMain {
  // Will be used to take back the context of item
  static GlobalKey id = GlobalKey();
  static int step = 0; // Increment for images

  static Future<void> takeScreenshot(String name) async {
    int currentStep = step++;
    final boundary = id.currentContext!.findRenderObject() 
          as RenderRepaintBoundary;
    ui.Image image = await boundary.toImage(pixelRatio: 3.0);
    // Convert image to bytes
    ByteData? bytes = await image.toByteData(
        format: ui.ImageByteFormat.png);
    Uint8List imageData = bytes!.buffer.asUint8List();
    File file = File('${Directory.current.absolute.path}/coverage/${currentStep.toString().padLeft(8, '0')}_$name.png');
    // "Sync" to avoid an infinite freeze during a test execution
    if (file.existsSync()) {
      file.deleteSync();
    }
    file.createSync(recursive: true, exclusive: false);
    file.writeAsBytesSync(imageData);
  }
  Future<void> initMain(WidgetTester tester, bool isIntegration) async {
    await tester.pumpWidget(MultiProvider(
      providers: [
        ChangeNotifierProvider<AppData>(
          create: (_) => getStore(isIntegration),
        ),
        ChangeNotifierProvider<AppTheme>(
          create: (_) => AppTheme(ThemeMode.system),
        ),
        ChangeNotifierProvider<AppLocale>(
          create: (_) => AppLocale(AppLocale.fromCode('en')),
        ),
      ],
      // Wrapper for a screen capturing
      child: RepaintBoundary(
        key: id,
        child: const MyApp(),
      ),
    ));
  }
}
\end{lstlisting}

\noindent Starting from that, we may do a screening simply by adding next command into the test (without \q{await}, 
again, to avoid an infinite freeze for the test execution, \cref{img:bdd-visualization}):

\begin{lstlisting}[language=terminal]
PumpMain.takeScreenshot(runtimeType.toString());
\end{lstlisting}  

\img{prototyping/bdd-visualization}{Usage of "takeScreenshot" to trace the problem}{img:bdd-visualization}

\noindent Certainly, the appearance of our application in its original state isn't exactly replicated, as processed in a 
simplified manner of rendering (like the Virtual DOM in React). However, the same method provides original screenshots 
for integration tests (\cref{int-tests}).


\subsubsection{Asserting by Images} \label{golden-image}

After successfully capturing images (\cref{t-record}), their utility extends beyond debugging alone by serving as 
golden images. These images can be used as benchmark references, facilitating the process of comparing visual 
outputs during subsequent testing iterations. By capturing and preserving the visual representation of a desired state, 
these images allow developers to discern any deviations or anomalies in the UI rendering as the application evolves. 

Moreover, this approach is instrumental in enabling the application to undergo across expected variability of screen 
sizes and pixel ratios, thereby ensuring a comprehensive assessment of its adaptive design alignments 
\issue{179}{b1873c2}. To generate images (\cref{img:t-images}) for a further assertion it's used \q{test}-command with 
an extra \q{update-goldens}-parameter, since the tests' images verification and generation is solely controlled by
\q{matchesGoldenFile}-method (a part of \q{flutter\_test}-package):

\begin{lstlisting}[language=terminal]
$ flutter test --update-goldens test/widget
\end{lstlisting}

\begin{lstlisting}
// test/widget/chart/painter/foreground_chart_painter_test.dart
testWidgets('Foreground Chart', (WidgetTester tester) async {
  // Register functions to be post-evaluated
  addTearDown(() {
    tester.view.resetPhysicalSize();
    tester.view.resetDevicePixelRatio();
  });
  // Update size and pixel ratio
  tester.view.physicalSize = const Size(320, 240);
  tester.view.devicePixelRatio = 1;
  // Init widget
  await tester.pumpWidget(widget);
  // expectLater is used for async matchers
  await expectLater( // Asset taken results with image
    find.byType(CustomPaint),
    matchesGoldenFile('./painter_test.dart.$i.png'),
    // Make assert the platform oriented
    skip: !Platform.isWindows,
  );
}
\end{lstlisting}

\img{prototyping/golden-images}{Golden Image generation assert}{img:t-images}
\img{prototyping/build-assert}{Sample of inconsistency between environments}{img:t-assert}

\noindent The absence of fonts discernible from the captured image earlier (\cref{img:bdd-visualization}), since 
\q{Foo}-font family is used by default in tests. To address this, we might use \q{FontLoader}-class to preload all 
needed fonts (\cref{img:t-images}). That approach holds the potential to enhance the overall visualization aspect of 
the tests:

\begin{lstlisting}
final asset = rootBundle.load('assets/fonts/Abel-Regular.ttf');
final font = FontLoader('Abel-Regular')..addFont(asset);
await font.load();
\end{lstlisting}

\noindent Ultimately, we have the option to construct golden tests with platform-specific attributes (as mentioned above 
to preempt errors, -- \cref{img:t-assert}), or alternatively, introduce a permissible threshold for allowable deviations 
(as described 
\href{https://github.com/flutter/flutter/pull/77014\#issuecomment-1048896776}{https://github.com/flutter/flutter/pull/77014}
or \href{https://stackoverflow.com/questions/62551504/flutter-golden-image-tests-diff-threshold}{https://stackoverflow.com/questions/62551504/}
and implemented in \issue{179}{c620a79}). This strategic flexibility ensures a balanced approach between stringent 
conformity and the inherent variability of UI rendering across different environments.
